{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_main.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "1NPtWQOSliV9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#from corpus import *\n",
        "#from utils import *\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.autograd as autograd\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler, WeightedRandomSampler\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import precision_recall_curve"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "olGnjl107LJl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Device selection"
      ]
    },
    {
      "metadata": {
        "id": "-dnu9vjq7NPX",
        "colab_type": "code",
        "outputId": "5684b56e-2e28-4156-85e9-1426c4bf5b80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "GPU = True\n",
        "device_idx = 0\n",
        "if GPU:\n",
        "    device = torch.device(\"cuda:\" + str(device_idx) if torch.cuda.is_available() else \"cpu\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)\n",
        "cpu = torch.device(\"cpu\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cs2G9W3D7OsZ",
        "colab_type": "code",
        "outputId": "bede2440-61f5-43f0-a060-0d6230d515a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# We set a random seed to ensure that your results are reproducible.\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f442041c190>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "ESC4kGIsmdtr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data management"
      ]
    },
    {
      "metadata": {
        "id": "HEsBqppJvfLw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Corpus management"
      ]
    },
    {
      "metadata": {
        "id": "kYUXEinNl8nb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#  <-------------- Dataset management -------------> #\n",
        "# Get the vocabulary back\n",
        "with open(\"vocabulary.txt\", \"r\") as file:\n",
        "    vocabulary = file.read().splitlines()\n",
        "\n",
        "# Get the cleaned tokenized corpus back\n",
        "with open(\"corpus_tweets.txt\", \"r\") as file:\n",
        "    tmp = file.read().splitlines()\n",
        "\n",
        "# In each sentence, we get rid of the last token, which is '\\n'\n",
        "clean_corpus = [sentence[:-1] for sentence in tmp]\n",
        "\n",
        "tokenized_corpus = [[token for token in sentence.split(' ')][:-1] for sentence in tmp]\n",
        "\n",
        "# Get the labels\n",
        "with open(\"labels_1.txt\", \"r\") as file:\n",
        "    label1 = file.read().splitlines()\n",
        "with open(\"labels_2.txt\", \"r\") as file:\n",
        "    label2 = file.read().splitlines()\n",
        "with open(\"labels_3.txt\", \"r\") as file:\n",
        "    label3 = file.read().splitlines()\n",
        "\n",
        "label1 = [float(i) for i in label1]\n",
        "label2 = [float(i) for i in label2]\n",
        "label3 = [float(i) for i in label3]\n",
        "\n",
        "#  <-------------- END Dataset management -------------> #\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H6xMi5nGvmHY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Word embedding from GloVe"
      ]
    },
    {
      "metadata": {
        "id": "E0VnqlAcnr3o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_path = 'emb_dic.txt'\n",
        "emb_dict = {}\n",
        "glove = open(embedding_path)\n",
        "for line in glove:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    try:\n",
        "        vector = np.asarray(values[1:], dtype='float32')\n",
        "        if len(vector) != 100:\n",
        "          print(word, len(vector))\n",
        "        emb_dict[word] = vector\n",
        "    except:\n",
        "        print(\"Parsing problem on word \", word, \" discarding it\")\n",
        "glove.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rWsgwj2zybw5",
        "colab_type": "code",
        "outputId": "e819d21a-4abd-4f30-83b3-e13b59295f2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(len(emb_dict['should']))"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "t-QNcAm9vVWy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data loaders"
      ]
    },
    {
      "metadata": {
        "id": "Yuu6Id53vYQA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def data_loader(emb_corpus, labels, batch_size, random_seed, valid_size=0.1, test_size=0.1, balancing=True):\n",
        "\n",
        "    # One hot encoding of labels, function in utils\n",
        "    #labels_ = one_hot_encoding(np.array(labels))\n",
        "    labels_ = torch.Tensor(labels)\n",
        "    # Sending everything to GPU\n",
        "    labels_.to(device)\n",
        "    emb_corpus.to(device)\n",
        "    \n",
        "    \n",
        "    # Create dataset\n",
        "    dataset_ = torch.utils.data.TensorDataset(emb_corpus, labels_)\n",
        "\n",
        "    # Split to train / valid / test dataset\n",
        "    size_dataset = len(labels)\n",
        "    indices = list(range(size_dataset))\n",
        "    valid_split = int(np.floor(valid_size * size_dataset))\n",
        "    test_split = int(np.floor(test_size * size_dataset))\n",
        "\n",
        "    np.random.seed(random_seed)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    train_valid_idx, test_idx = indices[test_split:], indices[:test_split]\n",
        "    train_idx, valid_idx = train_valid_idx[valid_split:], train_valid_idx[:valid_split]\n",
        "\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "    test_sampler = SubsetRandomSampler(test_idx)\n",
        "\n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        dataset_, batch_size=batch_size, sampler=valid_sampler)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        dataset_, batch_size=batch_size, sampler=test_sampler)\n",
        "\n",
        "    # We balancing set to False, we just create a data loader from the train indices\n",
        "    if not balancing:\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            dataset_, batch_size=batch_size, sampler=train_sampler)\n",
        "\n",
        "        return train_loader, valid_loader, test_loader\n",
        "\n",
        "    # If there is balancing to do, we first extract the training samples according to the\n",
        "    # predefined indices, before using a weighted sampler\n",
        "    if balancing:\n",
        "        train_loader_unbalanced = torch.utils.data.DataLoader(\n",
        "            dataset_, batch_size=len(labels), sampler=train_sampler)\n",
        "        \n",
        "        # Get back training data from sampler\n",
        "        training_data, training_labels = next(iter(train_loader_unbalanced))\n",
        "        # We get back classes from one hot encoding\n",
        "        #training_labels_int = training_labels.argmax(dim = 1, keepdim=True)\n",
        "        train_dataset = torch.utils.data.TensorDataset(training_data, training_labels)\n",
        "\n",
        "        # WeightedSampler takes the list of weights as input\n",
        "        class_sample_count = np.array([len(np.where(training_labels == t)[0]) for t in np.unique(training_labels)])\n",
        "        weight = 1. / class_sample_count\n",
        "        samples_weight = np.array([weight[int(t)] for t in training_labels])\n",
        "\n",
        "        samples_weight = torch.from_numpy(samples_weight)\n",
        "        samples_weight = samples_weight.double()\n",
        "        balance_sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
        "\n",
        "        train_loader_balanced = DataLoader(train_dataset, batch_size=batch_size, num_workers=1, sampler=balance_sampler)\n",
        "\n",
        "        return train_loader_balanced, valid_loader, test_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3KUywB8R0PxS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## First model : a 3 hidden layers fully connected feed forward network"
      ]
    },
    {
      "metadata": {
        "id": "XlWNDS76Nmyd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FFNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        embedding_dim = 100\n",
        "        h_dim1 = 512\n",
        "        h_dim2 = 256\n",
        "        h_dim3 = 64\n",
        "        num_classes = 1\n",
        "\n",
        "        # hidden layers\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Linear(in_features=embedding_dim, out_features=h_dim1, bias=True),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Linear(in_features=h_dim1, out_features=h_dim2, bias=True),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Linear(in_features=h_dim2, out_features=h_dim3, bias=True),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # output layer\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Linear(in_features=h_dim3, out_features=num_classes, bias=True),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_ZaI2QH25yh2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Some utils functions"
      ]
    },
    {
      "metadata": {
        "id": "5QAaQeMSpfj8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def accuracy(output, target):\n",
        "    correct = (output == target)\n",
        "    acc = float(float(correct.sum()) /len(output) ) * 100\n",
        "    return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K0A0qUwuEaE8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def recall_precision(cm):\n",
        "    nb_classes = cm.shape[0]\n",
        "    recall = np.zeros(nb_classes)\n",
        "    precision = np.zeros(nb_classes)\n",
        "    for idx in range(nb_classes):\n",
        "        if (idx + 1) < nb_classes:\n",
        "            false_negative = np.concatenate((cm[idx, :idx], cm[idx, (idx + 1):]))\n",
        "            false_positive = np.concatenate((cm[:idx, idx], cm[(idx + 1):, idx]))\n",
        "        else:\n",
        "            false_negative = cm[idx, :idx]\n",
        "            false_positive = cm[:idx, idx]\n",
        "        true_positive = cm[idx, idx]\n",
        "        recall[idx] = true_positive / (true_positive + false_negative.sum())\n",
        "        precision[idx] = true_positive / (true_positive + false_positive.sum())\n",
        "    return recall, precision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6RXptVQHqh0P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def eval(model, dataloader):\n",
        "\n",
        "  predictions = np.array(0)\n",
        "  targets = np.array(0)\n",
        "  \n",
        "  for batch_idx, (embedding, target) in enumerate(dataloader):    \n",
        "  \n",
        "    prediction = model(embedding.to(device)).detach()\n",
        "    predictions = np.append(predictions, np.round_(prediction.cpu().numpy()).astype(int))\n",
        "    \n",
        "    targets = np.append(targets, target.numpy().astype(int))\n",
        "  \n",
        "  return metrics(predictions, targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AYirWzYH21nG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def f1_measure(recall, precision):\n",
        "    f1 = 2 * (recall * precision) / (recall + precision)\n",
        "    return f1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oym0O1R_2ZdR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def metrics(prediction, target):\n",
        "    \n",
        "    # Number of correct predictions\n",
        "    acc = accuracy(prediction, target)\n",
        "    \n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(target, prediction)\n",
        "    \n",
        "    # Recall and precision\n",
        "    recall, precision = recall_precision(cm)\n",
        "    f1 = f1_measure(recall, precision)\n",
        "    \n",
        "    return acc, cm, recall, precision, f1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VY1C19a803Y8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def embed_corpus(emb_dict, corpus):\n",
        "    # Prepare container for tweet embeddings\n",
        "    inputs_ = torch.zeros((len(corpus), 100))\n",
        "\n",
        "    # Counter for debugging purposes\n",
        "    count_not_found = 0.\n",
        "    total_count = 0.\n",
        "\n",
        "    # We loop over all the tweets in the corpus\n",
        "    for idx, sentence in enumerate(corpus):\n",
        "        sentence_length = len(sentence)\n",
        "        mean_embedding = torch.zeros(100)\n",
        "        for word in sentence:\n",
        "            total_count += 1\n",
        "            if word in emb_dict.keys():\n",
        "                mean_embedding += torch.Tensor(emb_dict[word])\n",
        "            else:\n",
        "                count_not_found += 1\n",
        "\n",
        "        # We average the word embedding over the sentence\n",
        "        mean_embedding /= sentence_length\n",
        "\n",
        "        # We add the embedded sentence to the inputs tensor\n",
        "        inputs_[idx] = mean_embedding\n",
        "    ratio = (count_not_found / total_count) * 100\n",
        "\n",
        "    print(\"Percentage of not recognised words (those we do not have an embedding for) : %.2f\" % ratio, \"%\")\n",
        "    # We return the embedded corpus\n",
        "    return inputs_\n",
        "\n",
        "def one_hot_encoding(labels):\n",
        "    labels_ = torch.zeros(len(labels), int(np.amax(labels) + 1))\n",
        "    for idx, label in enumerate(labels):\n",
        "        labels_[idx, int(label)] = 1.\n",
        "    return labels_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ui7_b5AVnNCk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training"
      ]
    },
    {
      "metadata": {
        "id": "IQp4N0LqnDFc",
        "colab_type": "code",
        "outputId": "862c26ce-10e9-4e13-f7f9-7a0c7ed2532f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3488
        }
      },
      "cell_type": "code",
      "source": [
        "#  <----------- Global variables for the NN and the training --------------->\n",
        "\n",
        "# we will train for N epochs (N times the model will see all the data)\n",
        "epochs = 100\n",
        "\n",
        "#  <----------- END Global variables for the NN and the training --------------->\n",
        "\n",
        "emb_corpus = embed_corpus(emb_dict, tokenized_corpus)\n",
        "\n",
        "train_loader, valid_loader, test_loader = data_loader(emb_corpus, label1, 32, 1)\n",
        "\n",
        "# Instantiate the model\n",
        "model_FFNN = FFNN().to(device)\n",
        "\n",
        "# we use the stochastic gradient descent (SGD) optimizer\n",
        "optimizer = optim.SGD(model_FFNN.parameters(), lr=0.5)\n",
        "\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    loss_history = []\n",
        "    acc_history = []\n",
        "    for batch_idx, (embedding, target) in enumerate(train_loader):\n",
        "\n",
        "        model_FFNN.train()\n",
        "\n",
        "        # we zero the gradients as they are not removed automatically\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # squeeze is needed as the predictions are initially size (batch size, 1) and we need to remove the dimension of size 1\n",
        "        predictions = model_FFNN(embedding.to(device)).squeeze(1)\n",
        "        loss = nn.BCELoss()(predictions, target.to(device))\n",
        "        \n",
        "        # For log purposes\n",
        "        loss_history.append(float(loss))\n",
        "        predictions = predictions.detach()\n",
        "        target = target.detach()\n",
        "        acc_history.append(accuracy(np.round_(predictions.cpu().numpy()).astype(int), \n",
        "                                    target.cpu().numpy().astype(int)))\n",
        "\n",
        "        # calculate the gradient of each parameter\n",
        "        loss.backward()\n",
        "\n",
        "        # update the parameters using the gradients and optimizer algorithm\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = np.array(loss_history).mean()\n",
        "    epoch_acc = np.array(acc_history).mean()\n",
        "\n",
        "    val_acc, cm, recall, precision, f1 = eval(model_FFNN, valid_loader)\n",
        "    print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.3f} | Train Acc: {epoch_acc:.2f}%')\n",
        "    print(\"Valid accuracy :\", val_acc)"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of not recognised words (those we do not have an embedding for) : 3.35 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/utils/data/sampler.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.weights = torch.tensor(weights, dtype=torch.double)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| Epoch: 01 | Train Loss: 0.673 | Train Acc: 58.52%\n",
            "Valid accuracy : 42.49056603773585\n",
            "| Epoch: 02 | Train Loss: 0.636 | Train Acc: 64.00%\n",
            "Valid accuracy : 68.90566037735849\n",
            "| Epoch: 03 | Train Loss: 0.612 | Train Acc: 66.36%\n",
            "Valid accuracy : 68.90566037735849\n",
            "| Epoch: 04 | Train Loss: 0.604 | Train Acc: 67.22%\n",
            "Valid accuracy : 65.81132075471699\n",
            "| Epoch: 05 | Train Loss: 0.581 | Train Acc: 69.35%\n",
            "Valid accuracy : 74.18867924528301\n",
            "| Epoch: 06 | Train Loss: 0.584 | Train Acc: 68.76%\n",
            "Valid accuracy : 74.11320754716981\n",
            "| Epoch: 07 | Train Loss: 0.561 | Train Acc: 70.23%\n",
            "Valid accuracy : 63.62264150943396\n",
            "| Epoch: 08 | Train Loss: 0.569 | Train Acc: 69.97%\n",
            "Valid accuracy : 66.79245283018868\n",
            "| Epoch: 09 | Train Loss: 0.568 | Train Acc: 69.70%\n",
            "Valid accuracy : 70.79245283018868\n",
            "| Epoch: 10 | Train Loss: 0.564 | Train Acc: 70.42%\n",
            "Valid accuracy : 69.13207547169812\n",
            "| Epoch: 11 | Train Loss: 0.560 | Train Acc: 71.32%\n",
            "Valid accuracy : 74.64150943396226\n",
            "| Epoch: 12 | Train Loss: 0.555 | Train Acc: 71.38%\n",
            "Valid accuracy : 72.15094339622642\n",
            "| Epoch: 13 | Train Loss: 0.557 | Train Acc: 71.34%\n",
            "Valid accuracy : 71.24528301886792\n",
            "| Epoch: 14 | Train Loss: 0.553 | Train Acc: 71.15%\n",
            "Valid accuracy : 72.15094339622642\n",
            "| Epoch: 15 | Train Loss: 0.547 | Train Acc: 71.53%\n",
            "Valid accuracy : 67.09433962264151\n",
            "| Epoch: 16 | Train Loss: 0.553 | Train Acc: 71.12%\n",
            "Valid accuracy : 68.30188679245282\n",
            "| Epoch: 17 | Train Loss: 0.542 | Train Acc: 71.47%\n",
            "Valid accuracy : 69.58490566037736\n",
            "| Epoch: 18 | Train Loss: 0.547 | Train Acc: 71.87%\n",
            "Valid accuracy : 69.35849056603773\n",
            "| Epoch: 19 | Train Loss: 0.544 | Train Acc: 71.98%\n",
            "Valid accuracy : 73.50943396226415\n",
            "| Epoch: 20 | Train Loss: 0.542 | Train Acc: 71.57%\n",
            "Valid accuracy : 73.88679245283018\n",
            "| Epoch: 21 | Train Loss: 0.541 | Train Acc: 72.21%\n",
            "Valid accuracy : 70.18867924528301\n",
            "| Epoch: 22 | Train Loss: 0.542 | Train Acc: 72.05%\n",
            "Valid accuracy : 73.9622641509434\n",
            "| Epoch: 23 | Train Loss: 0.541 | Train Acc: 71.88%\n",
            "Valid accuracy : 70.71698113207547\n",
            "| Epoch: 24 | Train Loss: 0.530 | Train Acc: 73.05%\n",
            "Valid accuracy : 65.35849056603774\n",
            "| Epoch: 25 | Train Loss: 0.534 | Train Acc: 72.07%\n",
            "Valid accuracy : 65.50943396226415\n",
            "| Epoch: 26 | Train Loss: 0.536 | Train Acc: 72.31%\n",
            "Valid accuracy : 69.88679245283019\n",
            "| Epoch: 27 | Train Loss: 0.529 | Train Acc: 72.89%\n",
            "Valid accuracy : 73.81132075471697\n",
            "| Epoch: 28 | Train Loss: 0.533 | Train Acc: 72.32%\n",
            "Valid accuracy : 72.22641509433963\n",
            "| Epoch: 29 | Train Loss: 0.530 | Train Acc: 72.43%\n",
            "Valid accuracy : 59.62264150943396\n",
            "| Epoch: 30 | Train Loss: 0.516 | Train Acc: 73.51%\n",
            "Valid accuracy : 69.0566037735849\n",
            "| Epoch: 31 | Train Loss: 0.521 | Train Acc: 73.68%\n",
            "Valid accuracy : 71.54716981132076\n",
            "| Epoch: 32 | Train Loss: 0.518 | Train Acc: 73.42%\n",
            "Valid accuracy : 70.64150943396227\n",
            "| Epoch: 33 | Train Loss: 0.518 | Train Acc: 73.21%\n",
            "Valid accuracy : 72.0754716981132\n",
            "| Epoch: 34 | Train Loss: 0.521 | Train Acc: 73.31%\n",
            "Valid accuracy : 73.20754716981132\n",
            "| Epoch: 35 | Train Loss: 0.518 | Train Acc: 73.42%\n",
            "Valid accuracy : 72.0\n",
            "| Epoch: 36 | Train Loss: 0.512 | Train Acc: 73.89%\n",
            "Valid accuracy : 70.71698113207547\n",
            "| Epoch: 37 | Train Loss: 0.522 | Train Acc: 73.80%\n",
            "Valid accuracy : 73.43396226415095\n",
            "| Epoch: 38 | Train Loss: 0.515 | Train Acc: 73.80%\n",
            "Valid accuracy : 68.83018867924528\n",
            "| Epoch: 39 | Train Loss: 0.503 | Train Acc: 74.10%\n",
            "Valid accuracy : 71.32075471698113\n",
            "| Epoch: 40 | Train Loss: 0.512 | Train Acc: 73.68%\n",
            "Valid accuracy : 75.62264150943396\n",
            "| Epoch: 41 | Train Loss: 0.510 | Train Acc: 73.89%\n",
            "Valid accuracy : 55.77358490566038\n",
            "| Epoch: 42 | Train Loss: 0.513 | Train Acc: 74.11%\n",
            "Valid accuracy : 71.84905660377359\n",
            "| Epoch: 43 | Train Loss: 0.506 | Train Acc: 73.99%\n",
            "Valid accuracy : 72.60377358490567\n",
            "| Epoch: 44 | Train Loss: 0.505 | Train Acc: 73.90%\n",
            "Valid accuracy : 58.0377358490566\n",
            "| Epoch: 45 | Train Loss: 0.498 | Train Acc: 74.11%\n",
            "Valid accuracy : 69.43396226415094\n",
            "| Epoch: 46 | Train Loss: 0.500 | Train Acc: 74.60%\n",
            "Valid accuracy : 73.43396226415095\n",
            "| Epoch: 47 | Train Loss: 0.499 | Train Acc: 73.93%\n",
            "Valid accuracy : 75.09433962264151\n",
            "| Epoch: 48 | Train Loss: 0.501 | Train Acc: 74.51%\n",
            "Valid accuracy : 67.62264150943396\n",
            "| Epoch: 49 | Train Loss: 0.489 | Train Acc: 74.91%\n",
            "Valid accuracy : 75.24528301886792\n",
            "| Epoch: 50 | Train Loss: 0.497 | Train Acc: 74.21%\n",
            "Valid accuracy : 72.52830188679246\n",
            "| Epoch: 51 | Train Loss: 0.489 | Train Acc: 74.88%\n",
            "Valid accuracy : 66.8679245283019\n",
            "| Epoch: 52 | Train Loss: 0.498 | Train Acc: 74.63%\n",
            "Valid accuracy : 68.22641509433961\n",
            "| Epoch: 53 | Train Loss: 0.488 | Train Acc: 74.99%\n",
            "Valid accuracy : 57.509433962264154\n",
            "| Epoch: 54 | Train Loss: 0.481 | Train Acc: 75.78%\n",
            "Valid accuracy : 68.0\n",
            "| Epoch: 55 | Train Loss: 0.484 | Train Acc: 75.68%\n",
            "Valid accuracy : 69.58490566037736\n",
            "| Epoch: 56 | Train Loss: 0.485 | Train Acc: 75.33%\n",
            "Valid accuracy : 72.67924528301887\n",
            "| Epoch: 57 | Train Loss: 0.477 | Train Acc: 75.97%\n",
            "Valid accuracy : 65.13207547169812\n",
            "| Epoch: 58 | Train Loss: 0.483 | Train Acc: 75.60%\n",
            "Valid accuracy : 72.52830188679246\n",
            "| Epoch: 59 | Train Loss: 0.480 | Train Acc: 75.87%\n",
            "Valid accuracy : 63.094339622641506\n",
            "| Epoch: 60 | Train Loss: 0.474 | Train Acc: 75.89%\n",
            "Valid accuracy : 73.9622641509434\n",
            "| Epoch: 61 | Train Loss: 0.467 | Train Acc: 76.58%\n",
            "Valid accuracy : 67.62264150943396\n",
            "| Epoch: 62 | Train Loss: 0.468 | Train Acc: 76.37%\n",
            "Valid accuracy : 59.54716981132076\n",
            "| Epoch: 63 | Train Loss: 0.473 | Train Acc: 76.27%\n",
            "Valid accuracy : 69.9622641509434\n",
            "| Epoch: 64 | Train Loss: 0.471 | Train Acc: 76.22%\n",
            "Valid accuracy : 60.301886792452834\n",
            "| Epoch: 65 | Train Loss: 0.471 | Train Acc: 76.09%\n",
            "Valid accuracy : 67.9245283018868\n",
            "| Epoch: 66 | Train Loss: 0.462 | Train Acc: 76.66%\n",
            "Valid accuracy : 74.41509433962264\n",
            "| Epoch: 67 | Train Loss: 0.465 | Train Acc: 76.89%\n",
            "Valid accuracy : 67.62264150943396\n",
            "| Epoch: 68 | Train Loss: 0.465 | Train Acc: 76.45%\n",
            "Valid accuracy : 67.16981132075472\n",
            "| Epoch: 69 | Train Loss: 0.461 | Train Acc: 77.31%\n",
            "Valid accuracy : 52.075471698113205\n",
            "| Epoch: 70 | Train Loss: 0.459 | Train Acc: 77.02%\n",
            "Valid accuracy : 70.79245283018868\n",
            "| Epoch: 71 | Train Loss: 0.455 | Train Acc: 76.99%\n",
            "Valid accuracy : 70.0377358490566\n",
            "| Epoch: 72 | Train Loss: 0.456 | Train Acc: 77.18%\n",
            "Valid accuracy : 64.37735849056604\n",
            "| Epoch: 73 | Train Loss: 0.452 | Train Acc: 77.29%\n",
            "Valid accuracy : 67.84905660377359\n",
            "| Epoch: 74 | Train Loss: 0.443 | Train Acc: 77.63%\n",
            "Valid accuracy : 70.26415094339623\n",
            "| Epoch: 75 | Train Loss: 0.445 | Train Acc: 77.92%\n",
            "Valid accuracy : 67.24528301886792\n",
            "| Epoch: 76 | Train Loss: 0.433 | Train Acc: 78.72%\n",
            "Valid accuracy : 66.9433962264151\n",
            "| Epoch: 77 | Train Loss: 0.444 | Train Acc: 77.60%\n",
            "Valid accuracy : 72.9056603773585\n",
            "| Epoch: 78 | Train Loss: 0.445 | Train Acc: 77.82%\n",
            "Valid accuracy : 72.37735849056604\n",
            "| Epoch: 79 | Train Loss: 0.440 | Train Acc: 78.12%\n",
            "Valid accuracy : 71.9245283018868\n",
            "| Epoch: 80 | Train Loss: 0.436 | Train Acc: 78.01%\n",
            "Valid accuracy : 69.13207547169812\n",
            "| Epoch: 81 | Train Loss: 0.431 | Train Acc: 78.07%\n",
            "Valid accuracy : 68.90566037735849\n",
            "| Epoch: 82 | Train Loss: 0.427 | Train Acc: 78.81%\n",
            "Valid accuracy : 71.39622641509435\n",
            "| Epoch: 83 | Train Loss: 0.444 | Train Acc: 77.37%\n",
            "Valid accuracy : 61.735849056603776\n",
            "| Epoch: 84 | Train Loss: 0.428 | Train Acc: 78.78%\n",
            "Valid accuracy : 61.13207547169811\n",
            "| Epoch: 85 | Train Loss: 0.431 | Train Acc: 78.64%\n",
            "Valid accuracy : 67.77358490566037\n",
            "| Epoch: 86 | Train Loss: 0.437 | Train Acc: 77.94%\n",
            "Valid accuracy : 68.45283018867924\n",
            "| Epoch: 87 | Train Loss: 0.424 | Train Acc: 79.09%\n",
            "Valid accuracy : 71.01886792452831\n",
            "| Epoch: 88 | Train Loss: 0.417 | Train Acc: 78.97%\n",
            "Valid accuracy : 71.77358490566039\n",
            "| Epoch: 89 | Train Loss: 0.421 | Train Acc: 79.24%\n",
            "Valid accuracy : 69.88679245283019\n",
            "| Epoch: 90 | Train Loss: 0.410 | Train Acc: 80.36%\n",
            "Valid accuracy : 68.67924528301886\n",
            "| Epoch: 91 | Train Loss: 0.411 | Train Acc: 79.55%\n",
            "Valid accuracy : 68.98113207547169\n",
            "| Epoch: 92 | Train Loss: 0.415 | Train Acc: 79.62%\n",
            "Valid accuracy : 71.09433962264151\n",
            "| Epoch: 93 | Train Loss: 0.417 | Train Acc: 79.12%\n",
            "Valid accuracy : 69.35849056603773\n",
            "| Epoch: 94 | Train Loss: 0.414 | Train Acc: 79.36%\n",
            "Valid accuracy : 69.0566037735849\n",
            "| Epoch: 95 | Train Loss: 0.401 | Train Acc: 79.94%\n",
            "Valid accuracy : 74.11320754716981\n",
            "| Epoch: 96 | Train Loss: 0.399 | Train Acc: 80.39%\n",
            "Valid accuracy : 73.20754716981132\n",
            "| Epoch: 97 | Train Loss: 0.403 | Train Acc: 79.91%\n",
            "Valid accuracy : 64.75471698113208\n",
            "| Epoch: 98 | Train Loss: 0.402 | Train Acc: 79.65%\n",
            "Valid accuracy : 70.71698113207547\n",
            "| Epoch: 99 | Train Loss: 0.407 | Train Acc: 80.15%\n",
            "Valid accuracy : 60.0\n",
            "| Epoch: 100 | Train Loss: 0.397 | Train Acc: 80.36%\n",
            "Valid accuracy : 72.9811320754717\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b72XBChQrl85",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test data"
      ]
    },
    {
      "metadata": {
        "id": "7bMBIhp7nZR5",
        "colab_type": "code",
        "outputId": "ace0d64c-6715-4d93-db1f-673ef273031e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "acc_test, cm_test, recall_test, precision_test, f1_test = eval(model_FFNN, test_loader)\n",
        "print(\"Accuracy on test dataset : %.2f\" % acc_test, \"%\")\n",
        "print(\"F1-measure on test dataset : \", f1_test)"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test dataset : 73.28 %\n",
            "F1-measure on test dataset :  [0.79225352 0.62579281]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jjj0fp393k_n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## A second model : CNN"
      ]
    },
    {
      "metadata": {
        "id": "MJv2d_h63oLv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As we can see above, the FFNN detects offensive tweets with an accuracy of roughly 73%, which is not too bad for this naive model. However, this came after an important amount of preprocessing. In this part, I'll try to improve this detection accuracy by using a different kind of neural network : a CNN."
      ]
    },
    {
      "metadata": {
        "id": "cR_ESPLH4Fen",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def embed_corpus_2(emb_dict, corpus):\n",
        "\n",
        "    tweet_lengths = [len(tweet) for tweet in corpus]\n",
        "    max_len = np.max(np.array(tweet_lengths))\n",
        "\n",
        "    # Prepare container for tweet embeddings\n",
        "    inputs_ = torch.zeros((len(corpus), max_len, 100))\n",
        "\n",
        "    # Counter for debugging purposes\n",
        "    count_not_found = 0.\n",
        "    total_count = 0.\n",
        "\n",
        "    # We loop over all the tweets in the corpus\n",
        "    for idx, tweet in enumerate(corpus):\n",
        "        # and over all the words in a tweet\n",
        "        for idx2, word in enumerate(tweet):\n",
        "            total_count += 1\n",
        "            if word in emb_dict.keys():\n",
        "                inputs_[idx, idx2] = torch.Tensor(emb_dict[word])\n",
        "            else:\n",
        "                count_not_found += 1\n",
        "    ratio = (count_not_found / total_count) * 100\n",
        "\n",
        "    print(\"Percentage of not recognised words (those we do not have an embedding for) : %.2f\" % ratio, \"%\")\n",
        "    # We return the embedded corpus\n",
        "    return inputs_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ytswTyxOAyrw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, out_channels, window_size, output_dim, dropout):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # in_channels -- 1 text channel\n",
        "        # out_channels -- the number of output channels\n",
        "        # kernel_size is (window size x embedding dim)\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(window_size, embedding_dim))\n",
        "\n",
        "        # the dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # the output layer\n",
        "        self.fc = nn.Linear(out_channels, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch size, max sent length, embedding dim)\n",
        "\n",
        "        # We unsqueeze one dimension to give space to the coming convolution channels\n",
        "        embedded = x.unsqueeze(1)\n",
        "\n",
        "        # (batch size, 1, max sent length, embedding dim)\n",
        "\n",
        "        feature_maps = self.conv(embedded)\n",
        "\n",
        "        # (batch size, n filters, max input length - window size +1)\n",
        "\n",
        "        feature_maps = feature_maps.squeeze(3)\n",
        "\n",
        "        feature_maps = F.relu(feature_maps)\n",
        "\n",
        "        # the max pooling layer\n",
        "        pooled = F.max_pool1d(feature_maps, feature_maps.shape[2])\n",
        "\n",
        "        pooled = pooled.squeeze(2)\n",
        "\n",
        "        # (batch size, n_filters)\n",
        "\n",
        "        dropped = self.dropout(pooled)\n",
        "\n",
        "        preds = self.fc(dropped)\n",
        "\n",
        "        preds = torch.sigmoid(preds)\n",
        "        \n",
        "        return preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IC_pLe87BL-z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training"
      ]
    },
    {
      "metadata": {
        "id": "Bp4BpU0QA87V",
        "colab_type": "code",
        "outputId": "9f01efec-79b0-4a3e-e4ac-3d7ef75ad50d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3488
        }
      },
      "cell_type": "code",
      "source": [
        "epochs=100\n",
        "\n",
        "EMBEDDING_DIM = 100\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "#the hyperparameters specific to CNN\n",
        "\n",
        "# we define the number of filters\n",
        "N_OUT_CHANNELS = 100\n",
        "\n",
        "# we define the window size\n",
        "WINDOW_SIZE = 1\n",
        "\n",
        "# we apply the dropout with the probability 0.5\n",
        "DROPOUT = 0.5\n",
        "\n",
        "model_CNN = CNN(EMBEDDING_DIM, N_OUT_CHANNELS, WINDOW_SIZE, OUTPUT_DIM, DROPOUT).to(device)\n",
        "\n",
        "optimizer = optim.SGD(model_CNN.parameters(), lr=0.01)\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "emb_corpus = embed_corpus_2(emb_dict, tokenized_corpus)\n",
        "train_loader, valid_loader, test_loader = data_loader(emb_corpus, label1, 32, 1)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    loss_history = []\n",
        "    acc_history = []\n",
        "    for batch_idx, (embedding, target) in enumerate(train_loader):\n",
        "\n",
        "        model_CNN.train()\n",
        "\n",
        "        # we zero the gradients as they are not removed automatically\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # squeeze is needed as the predictions are initially size (batch size, 1) and we need to remove the dimension of size 1\n",
        "        predictions = model_CNN(embedding.to(device)).squeeze(1)   \n",
        "        loss = nn.BCELoss()(predictions, target.to(device))\n",
        "        loss_history.append(float(loss))\n",
        "        \n",
        "        predictions = predictions.detach()\n",
        "        target = target.detach()\n",
        "        acc_history.append(accuracy(np.round_(predictions.cpu().numpy()).astype(int), \n",
        "                                    target.cpu().numpy().astype(int)))\n",
        "        \n",
        "        # calculate the gradient of each parameter\n",
        "        loss.backward()\n",
        "\n",
        "        # update the parameters using the gradients and optimizer algorithm\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = np.array(loss_history).mean()\n",
        "    epoch_acc = np.array(acc_history).mean()\n",
        "\n",
        "    val_acc, cm, recall, precision, f1 = eval(model_CNN, valid_loader)\n",
        "    print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.3f} | Train Acc: {epoch_acc:.2f}%')\n",
        "    print(f'---> Valid accuracy : {val_acc:.2f}%')"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of not recognised words (those we do not have an embedding for) : 3.35 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/utils/data/sampler.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.weights = torch.tensor(weights, dtype=torch.double)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| Epoch: 01 | Train Loss: 0.695 | Train Acc: 52.02%\n",
            "---> Valid accuracy : 52.91%\n",
            "| Epoch: 02 | Train Loss: 0.672 | Train Acc: 60.11%\n",
            "---> Valid accuracy : 63.02%\n",
            "| Epoch: 03 | Train Loss: 0.643 | Train Acc: 64.77%\n",
            "---> Valid accuracy : 67.25%\n",
            "| Epoch: 04 | Train Loss: 0.613 | Train Acc: 67.67%\n",
            "---> Valid accuracy : 69.43%\n",
            "| Epoch: 05 | Train Loss: 0.584 | Train Acc: 69.89%\n",
            "---> Valid accuracy : 69.51%\n",
            "| Epoch: 06 | Train Loss: 0.571 | Train Acc: 70.48%\n",
            "---> Valid accuracy : 75.55%\n",
            "| Epoch: 07 | Train Loss: 0.560 | Train Acc: 71.28%\n",
            "---> Valid accuracy : 73.96%\n",
            "| Epoch: 08 | Train Loss: 0.550 | Train Acc: 72.56%\n",
            "---> Valid accuracy : 73.89%\n",
            "| Epoch: 09 | Train Loss: 0.540 | Train Acc: 73.02%\n",
            "---> Valid accuracy : 74.19%\n",
            "| Epoch: 10 | Train Loss: 0.543 | Train Acc: 72.83%\n",
            "---> Valid accuracy : 74.11%\n",
            "| Epoch: 11 | Train Loss: 0.546 | Train Acc: 73.15%\n",
            "---> Valid accuracy : 74.11%\n",
            "| Epoch: 12 | Train Loss: 0.543 | Train Acc: 72.82%\n",
            "---> Valid accuracy : 74.04%\n",
            "| Epoch: 13 | Train Loss: 0.531 | Train Acc: 73.49%\n",
            "---> Valid accuracy : 73.21%\n",
            "| Epoch: 14 | Train Loss: 0.544 | Train Acc: 72.78%\n",
            "---> Valid accuracy : 75.92%\n",
            "| Epoch: 15 | Train Loss: 0.528 | Train Acc: 74.24%\n",
            "---> Valid accuracy : 75.55%\n",
            "| Epoch: 16 | Train Loss: 0.516 | Train Acc: 74.85%\n",
            "---> Valid accuracy : 74.04%\n",
            "| Epoch: 17 | Train Loss: 0.531 | Train Acc: 73.98%\n",
            "---> Valid accuracy : 76.23%\n",
            "| Epoch: 18 | Train Loss: 0.528 | Train Acc: 74.17%\n",
            "---> Valid accuracy : 74.64%\n",
            "| Epoch: 19 | Train Loss: 0.525 | Train Acc: 74.18%\n",
            "---> Valid accuracy : 75.17%\n",
            "| Epoch: 20 | Train Loss: 0.517 | Train Acc: 74.79%\n",
            "---> Valid accuracy : 75.77%\n",
            "| Epoch: 21 | Train Loss: 0.511 | Train Acc: 75.37%\n",
            "---> Valid accuracy : 76.60%\n",
            "| Epoch: 22 | Train Loss: 0.517 | Train Acc: 74.84%\n",
            "---> Valid accuracy : 74.57%\n",
            "| Epoch: 23 | Train Loss: 0.513 | Train Acc: 75.43%\n",
            "---> Valid accuracy : 75.62%\n",
            "| Epoch: 24 | Train Loss: 0.522 | Train Acc: 74.46%\n",
            "---> Valid accuracy : 75.25%\n",
            "| Epoch: 25 | Train Loss: 0.511 | Train Acc: 74.97%\n",
            "---> Valid accuracy : 74.26%\n",
            "| Epoch: 26 | Train Loss: 0.520 | Train Acc: 74.72%\n",
            "---> Valid accuracy : 75.25%\n",
            "| Epoch: 27 | Train Loss: 0.515 | Train Acc: 75.69%\n",
            "---> Valid accuracy : 73.81%\n",
            "| Epoch: 28 | Train Loss: 0.514 | Train Acc: 74.95%\n",
            "---> Valid accuracy : 74.57%\n",
            "| Epoch: 29 | Train Loss: 0.504 | Train Acc: 75.62%\n",
            "---> Valid accuracy : 75.55%\n",
            "| Epoch: 30 | Train Loss: 0.512 | Train Acc: 75.26%\n",
            "---> Valid accuracy : 75.09%\n",
            "| Epoch: 31 | Train Loss: 0.511 | Train Acc: 75.47%\n",
            "---> Valid accuracy : 75.40%\n",
            "| Epoch: 32 | Train Loss: 0.519 | Train Acc: 74.41%\n",
            "---> Valid accuracy : 75.77%\n",
            "| Epoch: 33 | Train Loss: 0.509 | Train Acc: 75.07%\n",
            "---> Valid accuracy : 75.40%\n",
            "| Epoch: 34 | Train Loss: 0.509 | Train Acc: 75.47%\n",
            "---> Valid accuracy : 76.38%\n",
            "| Epoch: 35 | Train Loss: 0.503 | Train Acc: 75.94%\n",
            "---> Valid accuracy : 76.91%\n",
            "| Epoch: 36 | Train Loss: 0.508 | Train Acc: 75.41%\n",
            "---> Valid accuracy : 75.32%\n",
            "| Epoch: 37 | Train Loss: 0.498 | Train Acc: 76.39%\n",
            "---> Valid accuracy : 76.45%\n",
            "| Epoch: 38 | Train Loss: 0.504 | Train Acc: 76.01%\n",
            "---> Valid accuracy : 75.47%\n",
            "| Epoch: 39 | Train Loss: 0.506 | Train Acc: 75.92%\n",
            "---> Valid accuracy : 74.42%\n",
            "| Epoch: 40 | Train Loss: 0.506 | Train Acc: 75.21%\n",
            "---> Valid accuracy : 76.08%\n",
            "| Epoch: 41 | Train Loss: 0.497 | Train Acc: 76.38%\n",
            "---> Valid accuracy : 76.45%\n",
            "| Epoch: 42 | Train Loss: 0.505 | Train Acc: 75.66%\n",
            "---> Valid accuracy : 75.32%\n",
            "| Epoch: 43 | Train Loss: 0.489 | Train Acc: 77.20%\n",
            "---> Valid accuracy : 75.02%\n",
            "| Epoch: 44 | Train Loss: 0.495 | Train Acc: 75.97%\n",
            "---> Valid accuracy : 76.08%\n",
            "| Epoch: 45 | Train Loss: 0.497 | Train Acc: 76.60%\n",
            "---> Valid accuracy : 75.09%\n",
            "| Epoch: 46 | Train Loss: 0.496 | Train Acc: 76.18%\n",
            "---> Valid accuracy : 76.91%\n",
            "| Epoch: 47 | Train Loss: 0.498 | Train Acc: 76.27%\n",
            "---> Valid accuracy : 75.17%\n",
            "| Epoch: 48 | Train Loss: 0.490 | Train Acc: 76.51%\n",
            "---> Valid accuracy : 75.09%\n",
            "| Epoch: 49 | Train Loss: 0.501 | Train Acc: 76.54%\n",
            "---> Valid accuracy : 75.32%\n",
            "| Epoch: 50 | Train Loss: 0.492 | Train Acc: 76.35%\n",
            "---> Valid accuracy : 74.49%\n",
            "| Epoch: 51 | Train Loss: 0.494 | Train Acc: 76.20%\n",
            "---> Valid accuracy : 75.40%\n",
            "| Epoch: 52 | Train Loss: 0.493 | Train Acc: 76.67%\n",
            "---> Valid accuracy : 76.23%\n",
            "| Epoch: 53 | Train Loss: 0.497 | Train Acc: 76.03%\n",
            "---> Valid accuracy : 74.26%\n",
            "| Epoch: 54 | Train Loss: 0.499 | Train Acc: 75.94%\n",
            "---> Valid accuracy : 75.40%\n",
            "| Epoch: 55 | Train Loss: 0.486 | Train Acc: 77.03%\n",
            "---> Valid accuracy : 75.02%\n",
            "| Epoch: 56 | Train Loss: 0.487 | Train Acc: 76.70%\n",
            "---> Valid accuracy : 75.70%\n",
            "| Epoch: 57 | Train Loss: 0.493 | Train Acc: 76.79%\n",
            "---> Valid accuracy : 76.53%\n",
            "| Epoch: 58 | Train Loss: 0.490 | Train Acc: 76.96%\n",
            "---> Valid accuracy : 75.25%\n",
            "| Epoch: 59 | Train Loss: 0.489 | Train Acc: 76.35%\n",
            "---> Valid accuracy : 74.57%\n",
            "| Epoch: 60 | Train Loss: 0.479 | Train Acc: 77.43%\n",
            "---> Valid accuracy : 75.17%\n",
            "| Epoch: 61 | Train Loss: 0.488 | Train Acc: 76.27%\n",
            "---> Valid accuracy : 74.49%\n",
            "| Epoch: 62 | Train Loss: 0.483 | Train Acc: 77.45%\n",
            "---> Valid accuracy : 74.34%\n",
            "| Epoch: 63 | Train Loss: 0.478 | Train Acc: 77.62%\n",
            "---> Valid accuracy : 74.72%\n",
            "| Epoch: 64 | Train Loss: 0.497 | Train Acc: 76.26%\n",
            "---> Valid accuracy : 76.83%\n",
            "| Epoch: 65 | Train Loss: 0.489 | Train Acc: 76.51%\n",
            "---> Valid accuracy : 76.08%\n",
            "| Epoch: 66 | Train Loss: 0.485 | Train Acc: 77.48%\n",
            "---> Valid accuracy : 75.92%\n",
            "| Epoch: 67 | Train Loss: 0.493 | Train Acc: 76.56%\n",
            "---> Valid accuracy : 75.70%\n",
            "| Epoch: 68 | Train Loss: 0.476 | Train Acc: 77.48%\n",
            "---> Valid accuracy : 76.15%\n",
            "| Epoch: 69 | Train Loss: 0.477 | Train Acc: 77.49%\n",
            "---> Valid accuracy : 76.00%\n",
            "| Epoch: 70 | Train Loss: 0.483 | Train Acc: 77.13%\n",
            "---> Valid accuracy : 75.62%\n",
            "| Epoch: 71 | Train Loss: 0.476 | Train Acc: 77.28%\n",
            "---> Valid accuracy : 75.62%\n",
            "| Epoch: 72 | Train Loss: 0.466 | Train Acc: 78.22%\n",
            "---> Valid accuracy : 77.81%\n",
            "| Epoch: 73 | Train Loss: 0.479 | Train Acc: 77.77%\n",
            "---> Valid accuracy : 76.75%\n",
            "| Epoch: 74 | Train Loss: 0.466 | Train Acc: 78.13%\n",
            "---> Valid accuracy : 74.04%\n",
            "| Epoch: 75 | Train Loss: 0.471 | Train Acc: 77.80%\n",
            "---> Valid accuracy : 75.77%\n",
            "| Epoch: 76 | Train Loss: 0.470 | Train Acc: 77.52%\n",
            "---> Valid accuracy : 75.32%\n",
            "| Epoch: 77 | Train Loss: 0.471 | Train Acc: 78.16%\n",
            "---> Valid accuracy : 75.85%\n",
            "| Epoch: 78 | Train Loss: 0.476 | Train Acc: 77.58%\n",
            "---> Valid accuracy : 77.28%\n",
            "| Epoch: 79 | Train Loss: 0.472 | Train Acc: 77.96%\n",
            "---> Valid accuracy : 75.62%\n",
            "| Epoch: 80 | Train Loss: 0.471 | Train Acc: 77.81%\n",
            "---> Valid accuracy : 75.55%\n",
            "| Epoch: 81 | Train Loss: 0.469 | Train Acc: 77.45%\n",
            "---> Valid accuracy : 75.32%\n",
            "| Epoch: 82 | Train Loss: 0.478 | Train Acc: 77.53%\n",
            "---> Valid accuracy : 76.45%\n",
            "| Epoch: 83 | Train Loss: 0.466 | Train Acc: 78.09%\n",
            "---> Valid accuracy : 75.92%\n",
            "| Epoch: 84 | Train Loss: 0.467 | Train Acc: 78.14%\n",
            "---> Valid accuracy : 75.17%\n",
            "| Epoch: 85 | Train Loss: 0.468 | Train Acc: 78.06%\n",
            "---> Valid accuracy : 75.47%\n",
            "| Epoch: 86 | Train Loss: 0.471 | Train Acc: 78.12%\n",
            "---> Valid accuracy : 74.72%\n",
            "| Epoch: 87 | Train Loss: 0.464 | Train Acc: 78.53%\n",
            "---> Valid accuracy : 75.25%\n",
            "| Epoch: 88 | Train Loss: 0.471 | Train Acc: 78.27%\n",
            "---> Valid accuracy : 74.72%\n",
            "| Epoch: 89 | Train Loss: 0.465 | Train Acc: 78.17%\n",
            "---> Valid accuracy : 76.91%\n",
            "| Epoch: 90 | Train Loss: 0.467 | Train Acc: 78.46%\n",
            "---> Valid accuracy : 75.70%\n",
            "| Epoch: 91 | Train Loss: 0.471 | Train Acc: 77.60%\n",
            "---> Valid accuracy : 74.64%\n",
            "| Epoch: 92 | Train Loss: 0.470 | Train Acc: 77.69%\n",
            "---> Valid accuracy : 76.75%\n",
            "| Epoch: 93 | Train Loss: 0.467 | Train Acc: 78.72%\n",
            "---> Valid accuracy : 75.40%\n",
            "| Epoch: 94 | Train Loss: 0.462 | Train Acc: 78.60%\n",
            "---> Valid accuracy : 76.30%\n",
            "| Epoch: 95 | Train Loss: 0.454 | Train Acc: 78.88%\n",
            "---> Valid accuracy : 74.72%\n",
            "| Epoch: 96 | Train Loss: 0.451 | Train Acc: 79.07%\n",
            "---> Valid accuracy : 75.02%\n",
            "| Epoch: 97 | Train Loss: 0.468 | Train Acc: 78.64%\n",
            "---> Valid accuracy : 75.92%\n",
            "| Epoch: 98 | Train Loss: 0.457 | Train Acc: 78.72%\n",
            "---> Valid accuracy : 76.23%\n",
            "| Epoch: 99 | Train Loss: 0.459 | Train Acc: 79.14%\n",
            "---> Valid accuracy : 75.40%\n",
            "| Epoch: 100 | Train Loss: 0.465 | Train Acc: 78.52%\n",
            "---> Valid accuracy : 75.55%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9Ye17EQUD6bj",
        "colab_type": "code",
        "outputId": "74c489e1-519c-4699-f3dd-d8eaa753e741",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "acc_test, cm_test, recall_test, precision_test, f1_test = eval(model_CNN, test_loader)\n",
        "print(\"Accuracy on test dataset : %.2f\" % acc_test, \"%\")\n",
        "print(\"F1-measure on test dataset : \", f1_test)"
      ],
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test dataset : 76.08 %\n",
            "F1-measure on test dataset :  [0.81708021 0.65430752]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iNIP4sbdGRMO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Conclusion : with this simple CNN model, we improved :\n",
        " \n",
        "\n",
        "*   The detection accuracy from 74% (FFNN) to roughly **76%**.\n",
        "*   The F1 values also improved quite well : from [0.79 ; 0.62] to **[0.81 ; 0.65]**\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "DsAtuKYXqfNM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## A third model : bidirectional LSTM"
      ]
    },
    {
      "metadata": {
        "id": "7WnIUEA_qjGA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BiLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, output_dim, dropout, seq_len):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.seq_len = seq_len\n",
        "        self.batch_size = 32\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,\n",
        "                            dropout=dropout, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.linear = nn.Linear(seq_len * hidden_dim * 2, output_dim)\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        # Before we've done anything, we dont have any hidden state.\n",
        "        # Refer to the Pytorch documentation to see exactly\n",
        "        # why they have this dimensionality.\n",
        "        # The axes semantics are (num_layers * num_directions, minibatch_size, hidden_dim)\n",
        "        return (autograd.Variable(torch.zeros(2, self.batch_size, self.hidden_dim).to(device)),\n",
        "                autograd.Variable(torch.zeros(2, self.batch_size, self.hidden_dim).to(device)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.shape[0] != self.batch_size:\n",
        "            pad = torch.zeros((self.batch_size - x.shape[0], x.shape[1], x.shape[2])).to(device)\n",
        "            x = torch.cat((x, pad), dim=0)\n",
        "        # Shape of x  torch.Size([32, 105, 100])\n",
        "        # Shape of LSTM out  torch.Size([32, 105, 40])\n",
        "        \n",
        "        # lstm out should be (seq_len, batch, num_directions * hidden_size)\n",
        "        # elements of self.hidden should be (num_layers * num_directions, batch, hidden_size)\n",
        "        \n",
        "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
        "        lstm_out = lstm_out.contiguous()\n",
        "        lstm_out = lstm_out.view(-1, self.seq_len * 2 * self.hidden_dim)\n",
        "\n",
        "        tag_space = self.linear(lstm_out)\n",
        "        tag_scores = torch.sigmoid(tag_space)\n",
        "        return tag_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-V6OV23omY38",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def eval_lstm(model, dataloader, batch_size):\n",
        "    \n",
        "    predictions = np.array(0)\n",
        "    targets = np.array(0)\n",
        "    \n",
        "    for batch_idx, (embedding, target) in enumerate(dataloader):\n",
        "        # With LSTM, we will have troubles if the batch size changes \n",
        "        # (for example on the last batch. We discard it)\n",
        "        if embedding.shape[0] != batch_size:\n",
        "            continue\n",
        "        \n",
        "        embedding = embedding.to(device)\n",
        "        target = target.to(device)\n",
        "        \n",
        "        prediction = model(embedding).detach().cpu()\n",
        "        \n",
        "        predictions = np.append(predictions, np.round_(prediction.cpu().numpy()).astype(int))\n",
        "        targets = np.append(targets, target.cpu().numpy().astype(int))\n",
        "    \n",
        "    return metrics(predictions, targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sYVWkhFdqto9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training"
      ]
    },
    {
      "metadata": {
        "id": "eh8E9d_2qsc4",
        "colab_type": "code",
        "outputId": "54fba006-a3ac-4fb0-b69d-19051b1cc35b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "epochs=10\n",
        "\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 10\n",
        "OUTPUT_DIM = 1\n",
        "DROPOUT = 0.5\n",
        "SEQ_LEN = 105\n",
        "\n",
        "model_BiLSTM = BiLSTM(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, DROPOUT, SEQ_LEN).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model_BiLSTM.parameters())\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "emb_corpus = embed_corpus_2(emb_dict, tokenized_corpus)\n",
        "train_loader, valid_loader, test_loader = data_loader(emb_corpus, label1, batch_size, 1)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    loss_history = []\n",
        "    acc_history = []\n",
        "    for batch_idx, (embedding, target) in enumerate(train_loader):\n",
        "        \n",
        "        model_BiLSTM.train()\n",
        "\n",
        "        # we zero the gradients as they are not removed automatically\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Also, we need to clear out the hidden state of the LSTM,\n",
        "        # detaching it from its history on the last instance.\n",
        "        model_BiLSTM.hidden = model_BiLSTM.init_hidden()\n",
        "\n",
        "        # Send input data to GPU\n",
        "        embedding = embedding.to(device)\n",
        "        \n",
        "        # squeeze is needed as the predictions are initially size (batch size, 1) and we need to remove the dimension of size 1\n",
        "        # Have to transpose batch and sequence dimensions for nn.LSTM\n",
        "        predictions = model_BiLSTM(embedding).squeeze(1)\n",
        "        loss = nn.BCELoss()(predictions, target.to(device))\n",
        "        loss_history.append(float(loss))\n",
        "        \n",
        "        predictions = predictions.detach()\n",
        "        target = target.detach()\n",
        "        acc_history.append(accuracy(np.round_(predictions.cpu().numpy()).astype(int), \n",
        "                                    target.cpu().numpy().astype(int)))       \n",
        "        \n",
        "        # calculate the gradient of each parameter\n",
        "        loss.backward()\n",
        "\n",
        "        # update the parameters using the gradients and optimizer algorithm\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = np.array(loss_history).mean()\n",
        "    epoch_acc = np.array(acc_history).mean()\n",
        "    \n",
        "    val_acc, cm, recall, precision, f1 = eval_lstm(model_BiLSTM, valid_loader, batch_size)\n",
        "    print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.3f} | Train Acc: {epoch_acc:.2f}%')\n",
        "    print(f'---> Valid accuracy : {val_acc:.2f}%')"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Percentage of not recognised words (those we do not have an embedding for) : 3.35 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/utils/data/sampler.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.weights = torch.tensor(weights, dtype=torch.double)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| Epoch: 01 | Train Loss: 0.637 | Train Acc: 62.68%\n",
            "---> Valid accuracy : 71.97%\n",
            "| Epoch: 02 | Train Loss: 0.532 | Train Acc: 73.46%\n",
            "---> Valid accuracy : 73.12%\n",
            "| Epoch: 03 | Train Loss: 0.490 | Train Acc: 76.73%\n",
            "---> Valid accuracy : 74.41%\n",
            "| Epoch: 04 | Train Loss: 0.469 | Train Acc: 78.17%\n",
            "---> Valid accuracy : 76.54%\n",
            "| Epoch: 05 | Train Loss: 0.457 | Train Acc: 79.26%\n",
            "---> Valid accuracy : 74.94%\n",
            "| Epoch: 06 | Train Loss: 0.427 | Train Acc: 80.75%\n",
            "---> Valid accuracy : 76.16%\n",
            "| Epoch: 07 | Train Loss: 0.424 | Train Acc: 80.89%\n",
            "---> Valid accuracy : 78.07%\n",
            "| Epoch: 08 | Train Loss: 0.402 | Train Acc: 82.39%\n",
            "---> Valid accuracy : 76.47%\n",
            "| Epoch: 09 | Train Loss: 0.387 | Train Acc: 83.36%\n",
            "---> Valid accuracy : 71.44%\n",
            "| Epoch: 10 | Train Loss: 0.372 | Train Acc: 84.35%\n",
            "---> Valid accuracy : 74.71%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1Z0SUrpAoWUh",
        "colab_type": "code",
        "outputId": "87760c03-1032-4a8b-a123-6caa7a2f2b8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "acc_test, cm_test, recall_test, precision_test, f1_test = eval_lstm(model_BiLSTM, test_loader, batch_size)\n",
        "print(\"Accuracy on test dataset : %.2f\" % acc_test, \"%\")\n",
        "print(\"F1-measure on test dataset : \", f1_test)"
      ],
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test dataset : 75.78 %\n",
            "F1-measure on test dataset :  [0.81554524 0.64745011]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5VkoXuNmzaQJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Conclusion : on only 10 epochs with a naive dense layer output, this model achieves almost the same accuracy as the CNN model. We also see that this model overfits very quickly. Let's try to combine Bi-LSTM and convolutions now."
      ]
    },
    {
      "metadata": {
        "id": "AHAOBXBZzmj-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Last model : Bi-directional LSTM + convolutions"
      ]
    },
    {
      "metadata": {
        "id": "94RH1IlUzrt2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BiLSTMConv(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, output_dim, dropout, seq_len, \n",
        "                 channels, window_size, batch_size):\n",
        "        super(BiLSTMConv, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.seq_len = seq_len\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,\n",
        "                            dropout=dropout, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden = self.init_hidden()\n",
        "        \n",
        "        self.conv = nn.Conv2d(in_channels=1, out_channels=channels, kernel_size=(window_size, 2 * hidden_dim))\n",
        "        \n",
        "        # the dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.linear = nn.Linear(channels, output_dim)\n",
        "        \n",
        "    def init_hidden(self):\n",
        "        # Before we've done anything, we dont have any hidden state.\n",
        "        # Refer to the Pytorch documentation to see exactly\n",
        "        # why they have this dimensionality.\n",
        "        # The axes semantics are (num_layers * num_directions, minibatch_size, hidden_dim)\n",
        "        return (autograd.Variable(torch.zeros(2, self.batch_size, self.hidden_dim).to(device)),\n",
        "                autograd.Variable(torch.zeros(2, self.batch_size, self.hidden_dim).to(device)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.shape[0] != self.batch_size:\n",
        "            pad = torch.zeros((self.batch_size - x.shape[0], x.shape[1], x.shape[2])).to(device)\n",
        "            x = torch.cat((x, pad), dim=0)\n",
        "        # Shape of x  torch.Size([32, 105, 100])\n",
        "        # Shape of LSTM out  torch.Size([32, 105, 40])\n",
        "        \n",
        "        # lstm out should be (seq_len, batch, num_directions * hidden_size)\n",
        "        # elements of self.hidden should be (num_layers * num_directions, batch, hidden_size)\n",
        "        \n",
        "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
        "        \n",
        "        #lstm_out = lstm_out.contiguous()\n",
        "        #lstm_out = lstm_out.view(-1, self.seq_len * 2 * self.hidden_dim)\n",
        "\n",
        "        # make space for convolution channels\n",
        "        lstm_out = lstm_out.unsqueeze(1)\n",
        "        lstm_out = F.relu(lstm_out)\n",
        "        \n",
        "        \n",
        "        conv_out = self.conv(lstm_out)\n",
        "        \n",
        "        conv_out = conv_out.squeeze(3)\n",
        "        \n",
        "        pooled = F.max_pool1d(conv_out, conv_out.shape[2])\n",
        "        \n",
        "        pooled = pooled.squeeze(2)\n",
        "        \n",
        "        # (batch size, n_filters)\n",
        "        dropped = self.dropout(pooled)\n",
        "        \n",
        "        preds = self.linear(dropped)\n",
        "        preds = torch.sigmoid(preds)\n",
        "\n",
        "        return preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dTp3Yglyd25l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training"
      ]
    },
    {
      "metadata": {
        "id": "N0wo00TCd4CE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "c6e04e5e-d5c2-4bb5-d6f3-7f6f081b95f4"
      },
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "epochs=10\n",
        "\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 10\n",
        "OUTPUT_DIM = 1\n",
        "DROPOUT = 0.5\n",
        "SEQ_LEN = 105\n",
        "CHANNELS = 16\n",
        "WINDOW_SIZE = 1\n",
        "\n",
        "model_BiLSTMConv = BiLSTMConv(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, DROPOUT, \n",
        "                   SEQ_LEN, CHANNELS, WINDOW_SIZE, batch_size).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model_BiLSTMConv.parameters())\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "emb_corpus = embed_corpus_2(emb_dict, tokenized_corpus)\n",
        "train_loader, valid_loader, test_loader = data_loader(emb_corpus, label1, batch_size, 1)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    loss_history = []\n",
        "    acc_history = []\n",
        "    for batch_idx, (embedding, target) in enumerate(train_loader):\n",
        "        \n",
        "        model_BiLSTMConv.train()\n",
        "\n",
        "        # we zero the gradients as they are not removed automatically\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Also, we need to clear out the hidden state of the LSTM,\n",
        "        # detaching it from its history on the last instance.\n",
        "        model_BiLSTMConv.hidden = model_BiLSTMConv.init_hidden()\n",
        "\n",
        "        # Send input data to GPU\n",
        "        embedding = embedding.to(device)\n",
        "        \n",
        "        # squeeze is needed as the predictions are initially size (batch size, 1) and we need to remove the dimension of size 1\n",
        "        # Have to transpose batch and sequence dimensions for nn.LSTM\n",
        "        predictions = model_BiLSTMConv(embedding).squeeze(1)\n",
        "        loss = nn.BCELoss()(predictions, target.to(device))\n",
        "        loss_history.append(float(loss))\n",
        "        \n",
        "        predictions = predictions.detach()\n",
        "        target = target.detach()\n",
        "        acc_history.append(accuracy(np.round_(predictions.cpu().numpy()).astype(int), \n",
        "                                    target.cpu().numpy().astype(int)))     \n",
        "\n",
        "        # calculate the gradient of each parameter\n",
        "        loss.backward()\n",
        "\n",
        "        # update the parameters using the gradients and optimizer algorithm\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = np.array(loss_history).mean()\n",
        "    epoch_acc = np.array(acc_history).mean()\n",
        "    \n",
        "    val_acc, cm, recall, precision, f1 = eval_lstm(model_BiLSTMConv, valid_loader, batch_size)\n",
        "    print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.3f} | Train Acc: {epoch_acc:.2f}%')\n",
        "    print(f'---> Valid accuracy : {val_acc:.2f}%')"
      ],
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Percentage of not recognised words (those we do not have an embedding for) : 3.35 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/utils/data/sampler.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.weights = torch.tensor(weights, dtype=torch.double)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| Epoch: 01 | Train Loss: 0.675 | Train Acc: 57.21%\n",
            "---> Valid accuracy : 60.55%\n",
            "| Epoch: 02 | Train Loss: 0.554 | Train Acc: 73.45%\n",
            "---> Valid accuracy : 74.41%\n",
            "| Epoch: 03 | Train Loss: 0.510 | Train Acc: 76.03%\n",
            "---> Valid accuracy : 75.55%\n",
            "| Epoch: 04 | Train Loss: 0.499 | Train Acc: 76.62%\n",
            "---> Valid accuracy : 76.69%\n",
            "| Epoch: 05 | Train Loss: 0.482 | Train Acc: 77.80%\n",
            "---> Valid accuracy : 75.55%\n",
            "| Epoch: 06 | Train Loss: 0.471 | Train Acc: 79.14%\n",
            "---> Valid accuracy : 76.85%\n",
            "| Epoch: 07 | Train Loss: 0.470 | Train Acc: 78.64%\n",
            "---> Valid accuracy : 75.48%\n",
            "| Epoch: 08 | Train Loss: 0.467 | Train Acc: 78.02%\n",
            "---> Valid accuracy : 77.91%\n",
            "| Epoch: 09 | Train Loss: 0.448 | Train Acc: 79.63%\n",
            "---> Valid accuracy : 76.62%\n",
            "| Epoch: 10 | Train Loss: 0.442 | Train Acc: 80.16%\n",
            "---> Valid accuracy : 77.30%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pjkJRPJNgSyR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "361f16f2-cdbb-43d0-a0e6-84f23db65109"
      },
      "cell_type": "code",
      "source": [
        "acc_test, cm_test, recall_test, precision_test, f1_test = eval_lstm(model_BiLSTMConv, test_loader, batch_size)\n",
        "print(\"Accuracy on test dataset : %.2f\" % acc_test, \"%\")\n",
        "print(\"F1-measure on test dataset : \", f1_test)"
      ],
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test dataset : 76.92 %\n",
            "F1-measure on test dataset :  [0.82455124 0.66295884]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iITP-M3aulii",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x_X5QqsHum8f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Test generation"
      ]
    },
    {
      "metadata": {
        "id": "vdCP44OSup-Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#  <-------------- Dataset management -------------> #\n",
        "# Get the vocabulary back\n",
        "with open(\"test_vocabulary_a.txt\", \"r\") as file:\n",
        "    vocabulary_a = file.read().splitlines()\n",
        "\n",
        "# Get the cleaned tokenized corpus back\n",
        "with open(\"test_corpus_tweets_a.txt\", \"r\") as file:\n",
        "    tmp_a = file.read().splitlines()\n",
        "\n",
        "# In each sentence, we get rid of the last token, which is '\\n'\n",
        "clean_corpus_a = [sentence[:-1] for sentence in tmp_a]\n",
        "\n",
        "tokenized_corpus_a = [[token for token in sentence.split(' ')][:-1] for sentence in tmp_a]\n",
        "\n",
        "#  <-------------- END Dataset management -------------> #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E5wc7pTCuzIf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_path = 'emb_dic_a.txt'\n",
        "emb_dict_a = {}\n",
        "glove = open(embedding_path)\n",
        "for line in glove:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    try:\n",
        "        vector = np.asarray(values[1:], dtype='float32')\n",
        "        if len(vector) != 100:\n",
        "          print(word, len(vector))\n",
        "        emb_dict_a[word] = vector\n",
        "    except:\n",
        "        print(\"Parsing problem on word \", word, \" discarding it\")\n",
        "glove.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F6FxH51Tuzyy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ede3bbc4-74f8-4b7b-f2fc-a3a980115e64"
      },
      "cell_type": "code",
      "source": [
        "emb_corpus_a = embed_corpus_2(emb_dict_a, tokenized_corpus_a)"
      ],
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of not recognised words (those we do not have an embedding for) : 7.47 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FtbT4E7gw1qI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cfaf5e56-2a54-49e8-babb-28d86e594d60"
      },
      "cell_type": "code",
      "source": [
        "print(emb_corpus_a.shape)\n",
        "\n",
        "# We need to make this a multiple of batch_size = 32. We padd with zeros. \n",
        "# We will discard the results after.\n",
        "\n",
        "sent_pad = torch.zeros((4, 66, 100))\n",
        "\n",
        "emb_corpus_a = torch.cat((emb_corpus_a, sent_pad), dim=0)\n",
        "\n",
        "print(emb_corpus_a.shape)"
      ],
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([860, 66, 100])\n",
            "torch.Size([864, 66, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yebMLN96vjwv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Make batches of 32 : \n",
        "\n",
        "input_a = emb_corpus_a.view(-1, 32, 66, 100)\n",
        "\n",
        "# Preparing a container for the results\n",
        "prediction_a = torch.zeros((int(864/32),32))\n",
        "\n",
        "for batch in range(26):\n",
        "    output = model_BiLSTMConv(torch.Tensor(input_a[batch]).to(device))\n",
        "    prediction_a[batch] = output.cpu().detach().squeeze(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SCXD4AjNz3Pp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "dbc7b0f9-b79f-45d1-d5e4-f8725a5db9da"
      },
      "cell_type": "code",
      "source": [
        "# reshape prediction_a and dicard the last elements corresponding to the padding\n",
        "prediction_a = prediction_a.view(-1)[:-4]\n",
        "print(prediction_a.shape)\n",
        "\n",
        "# Getting back the np.array, and round the result to have 0 or 1\n",
        "prediction_a = prediction_a.numpy()\n",
        "prediction_a = np.round_(prediction_a).astype(int)\n",
        "print(prediction_a)"
      ],
      "execution_count": 282,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([860])\n",
            "[1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0\n",
            " 0 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0\n",
            " 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1\n",
            " 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1\n",
            " 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1\n",
            " 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1\n",
            " 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0\n",
            " 1 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0\n",
            " 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
            " 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 0 1\n",
            " 1 1 0 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 0\n",
            " 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0\n",
            " 1 0 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1\n",
            " 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1\n",
            " 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0\n",
            " 1 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0\n",
            " 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            " 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v3ejjgrI5uYG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "prediction_a_letters = [\"OFF\" if element == 1 else \"NOT\" for element in prediction_a]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HHxdaHJK5-BA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataframe_a = pd.read_csv('testset-taska.tsv', sep=\"\\t\", header=0)\n",
        "id_a = dataframe_a[\"id\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qsJVBZdf666F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pred_dataframe = pd.DataFrame(prediction_a_letters, index=id_a)\n",
        "pred_dataframe.to_csv(path_or_buf =\"pred_a.csv\", header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2ypSV0zmuODz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sub task b"
      ]
    },
    {
      "metadata": {
        "id": "Q3IwfPVDuPwm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#  <-------------- Dataset management -------------> #\n",
        "# Get the vocabulary back\n",
        "with open(\"vocabulary.txt\", \"r\") as file:\n",
        "    vocabulary_a = file.read().splitlines()\n",
        "\n",
        "# Get the cleaned tokenized corpus back\n",
        "with open(\"corpus_tweets_subtask_b.txt\", \"r\") as file:\n",
        "    tmp_b = file.read().splitlines()\n",
        "\n",
        "# In each sentence, we get rid of the last token, which is '\\n'\n",
        "clean_corpus_b = [sentence[:-1] for sentence in tmp_b]\n",
        "\n",
        "tokenized_corpus_b = [[token for token in sentence.split(' ')][:-1] for sentence in tmp_b]\n",
        "\n",
        "#  <-------------- END Dataset management -------------> #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W4f-FHBZvrrO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Get the labels\n",
        "with open(\"labels_subtask_b.txt\", \"r\") as file:\n",
        "    label_b = file.read().splitlines()\n",
        "\n",
        "\n",
        "label_b = [float(i) for i in label_b]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HPQ3CRdrun90",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_path = 'emb_dic.txt'\n",
        "emb_dict = {}\n",
        "glove = open(embedding_path)\n",
        "for line in glove:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    try:\n",
        "        vector = np.asarray(values[1:], dtype='float32')\n",
        "        if len(vector) != 100:\n",
        "          print(word, len(vector))\n",
        "        emb_dict[word] = vector\n",
        "    except:\n",
        "        print(\"Parsing problem on word \", word, \" discarding it\")\n",
        "glove.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1qzQQbB3u34n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0cc22c71-a6dd-44b2-f36c-0f2acd7d5fda"
      },
      "cell_type": "code",
      "source": [
        "emb_corpus_b = embed_corpus_2(emb_dict, tokenized_corpus_b)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of not recognised words (those we do not have an embedding for) : 2.84 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "u7CZ6wTLvbSr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training"
      ]
    },
    {
      "metadata": {
        "id": "dimH5AB1vc1Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "f0d9d445-4e2d-4164-c82d-96645bc4bf31"
      },
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "epochs=15\n",
        "\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 10\n",
        "OUTPUT_DIM = 1\n",
        "DROPOUT = 0.5\n",
        "SEQ_LEN = 105\n",
        "CHANNELS = 16\n",
        "WINDOW_SIZE = 1\n",
        "\n",
        "model_BiLSTMConv = BiLSTMConv(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, DROPOUT, \n",
        "                   SEQ_LEN, CHANNELS, WINDOW_SIZE, batch_size).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model_BiLSTMConv.parameters())\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "train_loader, valid_loader, test_loader = data_loader(emb_corpus_b, label_b, batch_size, 1, valid_size=0, test_size=0)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    loss_history = []\n",
        "    acc_history = []\n",
        "    for batch_idx, (embedding, target) in enumerate(train_loader):\n",
        "        \n",
        "        model_BiLSTMConv.train()\n",
        "\n",
        "        # we zero the gradients as they are not removed automatically\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Also, we need to clear out the hidden state of the LSTM,\n",
        "        # detaching it from its history on the last instance.\n",
        "        model_BiLSTMConv.hidden = model_BiLSTMConv.init_hidden()\n",
        "\n",
        "        # Send input data to GPU\n",
        "        embedding = embedding.to(device)\n",
        "        \n",
        "        # squeeze is needed as the predictions are initially size (batch size, 1) and we need to remove the dimension of size 1\n",
        "        # Have to transpose batch and sequence dimensions for nn.LSTM\n",
        "        predictions = model_BiLSTMConv(embedding).squeeze(1)\n",
        "        loss = nn.BCELoss()(predictions, target.to(device))\n",
        "        loss_history.append(float(loss))\n",
        "        \n",
        "        predictions = predictions.detach()\n",
        "        target = target.detach()\n",
        "        acc_history.append(accuracy(np.round_(predictions.cpu().numpy()).astype(int), \n",
        "                                    target.cpu().numpy().astype(int)))     \n",
        "\n",
        "        # calculate the gradient of each parameter\n",
        "        loss.backward()\n",
        "\n",
        "        # update the parameters using the gradients and optimizer algorithm\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = np.array(loss_history).mean()\n",
        "    epoch_acc = np.array(acc_history).mean()\n",
        "\n",
        "    #val_acc, cm, recall, precision, f1 = eval_lstm(model_BiLSTMConv, valid_loader, batch_size)\n",
        "    print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.3f} | Train Acc: {epoch_acc:.2f}%')\n",
        "    #print(f'---> Valid accuracy : {val_acc:.2f}%')"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/utils/data/sampler.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.weights = torch.tensor(weights, dtype=torch.double)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| Epoch: 01 | Train Loss: 0.692 | Train Acc: 52.89%\n",
            "| Epoch: 02 | Train Loss: 0.658 | Train Acc: 61.93%\n",
            "| Epoch: 03 | Train Loss: 0.623 | Train Acc: 65.75%\n",
            "| Epoch: 04 | Train Loss: 0.586 | Train Acc: 71.25%\n",
            "| Epoch: 05 | Train Loss: 0.549 | Train Acc: 74.14%\n",
            "| Epoch: 06 | Train Loss: 0.524 | Train Acc: 75.27%\n",
            "| Epoch: 07 | Train Loss: 0.482 | Train Acc: 78.43%\n",
            "| Epoch: 08 | Train Loss: 0.457 | Train Acc: 80.00%\n",
            "| Epoch: 09 | Train Loss: 0.417 | Train Acc: 82.00%\n",
            "| Epoch: 10 | Train Loss: 0.364 | Train Acc: 85.59%\n",
            "| Epoch: 11 | Train Loss: 0.330 | Train Acc: 87.30%\n",
            "| Epoch: 12 | Train Loss: 0.295 | Train Acc: 89.89%\n",
            "| Epoch: 13 | Train Loss: 0.265 | Train Acc: 90.57%\n",
            "| Epoch: 14 | Train Loss: 0.230 | Train Acc: 92.34%\n",
            "| Epoch: 15 | Train Loss: 0.197 | Train Acc: 93.50%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DC9AjNw8wFm0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8c1ccdd7-d440-4441-de20-1c64ed439fcc"
      },
      "cell_type": "code",
      "source": [
        "acc_test, cm_test, recall_test, precision_test, f1_test = eval_lstm(model_BiLSTMConv, test_loader, batch_size)\n",
        "print(\"Accuracy on test dataset : %.2f\" % acc_test, \"%\")\n",
        "print(\"F1-measure on test dataset : \", f1_test)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test dataset : 79.21 %\n",
            "F1-measure on test dataset :  [0.28571429 0.87837838]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fNLCRCkpzx0h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Test sub_task_b"
      ]
    },
    {
      "metadata": {
        "id": "LuNeHW3kz0xx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#  <-------------- Dataset management -------------> #\n",
        "# Get the vocabulary back\n",
        "with open(\"test_vocabulary_b.txt\", \"r\") as file:\n",
        "    vocabulary_b = file.read().splitlines()\n",
        "\n",
        "# Get the cleaned tokenized corpus back\n",
        "with open(\"test_corpus_tweets_b.txt\", \"r\") as file:\n",
        "    tmp_b = file.read().splitlines()\n",
        "\n",
        "# In each sentence, we get rid of the last token, which is '\\n'\n",
        "clean_corpus_b = [sentence[:-1] for sentence in tmp_b]\n",
        "\n",
        "tokenized_corpus_b = [[token for token in sentence.split(' ')][:-1] for sentence in tmp_b]\n",
        "\n",
        "#  <-------------- END Dataset management -------------> #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ztnVx-bH1dVl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_path = 'emb_dic_b.txt'\n",
        "emb_dict_b = {}\n",
        "glove = open(embedding_path)\n",
        "for line in glove:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    try:\n",
        "        vector = np.asarray(values[1:], dtype='float32')\n",
        "        if len(vector) != 100:\n",
        "          print(word, len(vector))\n",
        "        emb_dict_b[word] = vector\n",
        "    except:\n",
        "        print(\"Parsing problem on word \", word, \" discarding it\")\n",
        "glove.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bbbbStoY1OQC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5bb933ad-ede2-47f5-fa4e-3e660e3bdb8d"
      },
      "cell_type": "code",
      "source": [
        "emb_corpus_b = embed_corpus_2(emb_dict_b, tokenized_corpus_b)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of not recognised words (those we do not have an embedding for) : 6.37 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W9br2-j21mWq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a7925743-80bd-4065-94a7-368ba03d6503"
      },
      "cell_type": "code",
      "source": [
        "print(emb_corpus_b.shape)\n",
        "\n",
        "# It is already a mutliple of batchsize 16"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([240, 59, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_rrPtHw5132K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Make batches of 16 : \n",
        "\n",
        "input_b = emb_corpus_b.view(-1, 16, 59, 100)\n",
        "\n",
        "# Preparing a container for the results\n",
        "prediction_b = torch.zeros((int(240/16),16))\n",
        "\n",
        "for batch in range(15):\n",
        "    output = model_BiLSTMConv(torch.Tensor(input_b[batch]).to(device))\n",
        "    prediction_b[batch] = output.cpu().detach().squeeze(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_XNi1h5I2J7o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "ca18c034-2c89-494a-eebd-d6da2208b2a9"
      },
      "cell_type": "code",
      "source": [
        "# reshape prediction_a and dicard the last elements corresponding to the padding\n",
        "prediction_b = prediction_b.view(-1)\n",
        "print(prediction_b.shape)\n",
        "\n",
        "# Getting back the np.array, and round the result to have 0 or 1\n",
        "prediction_b = prediction_b.numpy()\n",
        "prediction_b = np.round_(prediction_b).astype(int)\n",
        "print(prediction_b)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([240])\n",
            "[1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0\n",
            " 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0\n",
            " 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1\n",
            " 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p6qK7-HR2X8r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "prediction_b_letters = [\"TIN\" if element == 1 else \"UNT\" for element in prediction_b]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pnbO0LUp2d98",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataframe_b = pd.read_csv('testset-taskb.tsv', sep=\"\\t\", header=0)\n",
        "id_b = dataframe_b[\"id\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5G_vHkhR2xq7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pred_dataframe = pd.DataFrame(prediction_b_letters, index=id_b)\n",
        "pred_dataframe.to_csv(path_or_buf =\"pred_b.csv\", header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-GFIHcYN9rtg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sub task C"
      ]
    },
    {
      "metadata": {
        "id": "7F5ps54U9ts9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will operate a slight modification to the network to have 4 neurons in the output layer instead of 1. We will have to change the activation function (from ReLU() to Softmax()) and adapt the Loss function too."
      ]
    },
    {
      "metadata": {
        "id": "mAU_hrR2BNk-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#  <-------------- Dataset management -------------> #\n",
        "# Get the vocabulary back\n",
        "with open(\"vocabulary.txt\", \"r\") as file:\n",
        "    vocabulary_a = file.read().splitlines()\n",
        "\n",
        "# Get the cleaned tokenized corpus back\n",
        "with open(\"corpus_tweets_subtask_c.txt\", \"r\") as file:\n",
        "    tmp_c = file.read().splitlines()\n",
        "\n",
        "# In each sentence, we get rid of the last token, which is '\\n'\n",
        "clean_corpus_c = [sentence[:-1] for sentence in tmp_c]\n",
        "\n",
        "tokenized_corpus_c = [[token for token in sentence.split(' ')][:-1] for sentence in tmp_c]\n",
        "\n",
        "#  <-------------- END Dataset management -------------> #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zh0ESzVTBYJg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Get the labels\n",
        "with open(\"labels_subtask_c.txt\", \"r\") as file:\n",
        "    label_c = file.read().splitlines()\n",
        "\n",
        "label_c = [float(i) for i in label_c]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F4GT3XjiBkJ4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_path = 'emb_dic.txt'\n",
        "emb_dict = {}\n",
        "glove = open(embedding_path)\n",
        "for line in glove:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    try:\n",
        "        vector = np.asarray(values[1:], dtype='float32')\n",
        "        if len(vector) != 100:\n",
        "          print(word, len(vector))\n",
        "        emb_dict[word] = vector\n",
        "    except:\n",
        "        print(\"Parsing problem on word \", word, \" discarding it\")\n",
        "glove.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oflX06LQBvSZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c7c44e19-709d-4fe8-b387-3c1eb07b9e43"
      },
      "cell_type": "code",
      "source": [
        "emb_corpus_c = embed_corpus_2(emb_dict, tokenized_corpus_c)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of not recognised words (those we do not have an embedding for) : 2.79 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Qgcwvo8g-AC8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BiLSTMConv4(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, output_dim, dropout, seq_len, \n",
        "                 channels, window_size, batch_size):\n",
        "        super(BiLSTMConv, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.seq_len = seq_len\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,\n",
        "                            dropout=dropout, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden = self.init_hidden()\n",
        "        \n",
        "        self.conv = nn.Conv2d(in_channels=1, out_channels=channels, kernel_size=(window_size, 2 * hidden_dim))\n",
        "        \n",
        "        # the dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.linear = nn.Linear(channels, output_dim)\n",
        "        \n",
        "    def init_hidden(self):\n",
        "        # Before we've done anything, we dont have any hidden state.\n",
        "        # Refer to the Pytorch documentation to see exactly\n",
        "        # why they have this dimensionality.\n",
        "        # The axes semantics are (num_layers * num_directions, minibatch_size, hidden_dim)\n",
        "        return (autograd.Variable(torch.zeros(2, self.batch_size, self.hidden_dim).to(device)),\n",
        "                autograd.Variable(torch.zeros(2, self.batch_size, self.hidden_dim).to(device)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.shape[0] != self.batch_size:\n",
        "            pad = torch.zeros((self.batch_size - x.shape[0], x.shape[1], x.shape[2])).to(device)\n",
        "            x = torch.cat((x, pad), dim=0)\n",
        "        # Shape of x  torch.Size([32, 105, 100])\n",
        "        # Shape of LSTM out  torch.Size([32, 105, 40])\n",
        "        \n",
        "        # lstm out should be (seq_len, batch, num_directions * hidden_size)\n",
        "        # elements of self.hidden should be (num_layers * num_directions, batch, hidden_size)\n",
        "        \n",
        "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
        "        \n",
        "        #lstm_out = lstm_out.contiguous()\n",
        "        #lstm_out = lstm_out.view(-1, self.seq_len * 2 * self.hidden_dim)\n",
        "\n",
        "        # make space for convolution channels\n",
        "        lstm_out = lstm_out.unsqueeze(1)\n",
        "        lstm_out = F.relu(lstm_out)\n",
        "        \n",
        "        \n",
        "        conv_out = self.conv(lstm_out)\n",
        "        \n",
        "        conv_out = conv_out.squeeze(3)\n",
        "        \n",
        "        pooled = F.max_pool1d(conv_out, conv_out.shape[2])\n",
        "        \n",
        "        pooled = pooled.squeeze(2)\n",
        "        \n",
        "        # (batch size, n_filters)\n",
        "        dropped = self.dropout(pooled)\n",
        "        \n",
        "        preds = self.linear(dropped)\n",
        "\n",
        "        return preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_8nrEFEE_GHq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def eval_lstm2(model, dataloader, batch_size):\n",
        "    \n",
        "    predictions = np.array(0)\n",
        "    targets = np.array(0)\n",
        "    \n",
        "    for batch_idx, (embedding, target) in enumerate(dataloader):\n",
        "        # With LSTM, we will have troubles if the batch size changes \n",
        "        # (for example on the last batch. We discard it)\n",
        "        if embedding.shape[0] != batch_size:\n",
        "            continue\n",
        "        \n",
        "        embedding = embedding.to(device)\n",
        "        target = target.to(device)\n",
        "        \n",
        "        # Here we have to take the softmax of the predictions first\n",
        "        prediction = torch.softmax(model(embedding).detach().cpu(), dim=1)\n",
        "        prediction = prediction.cpu().numpy()\n",
        "        prediction = np.argmax(prediction, axis=1)\n",
        "        \n",
        "        predictions = np.append(predictions, prediction.astype(int))\n",
        "        targets = np.append(targets, target.cpu().numpy().astype(int))\n",
        "    \n",
        "    return metrics(predictions, targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eFG5_Iwu-Xb3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "cd69d15d-f3e5-4075-e3ff-076bdbc43109"
      },
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "epochs=15\n",
        "\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 10\n",
        "OUTPUT_DIM = 3\n",
        "DROPOUT = 0.5\n",
        "SEQ_LEN = 105\n",
        "CHANNELS = 16\n",
        "WINDOW_SIZE = 1\n",
        "\n",
        "model_BiLSTMConv = BiLSTMConv(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, DROPOUT, \n",
        "                   SEQ_LEN, CHANNELS, WINDOW_SIZE, batch_size).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model_BiLSTMConv.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "train_loader, valid_loader, test_loader = data_loader(emb_corpus_c, label_c, batch_size, 1)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    loss_history = []\n",
        "    acc_history = []\n",
        "    for batch_idx, (embedding, target) in enumerate(train_loader):\n",
        "        \n",
        "        model_BiLSTMConv.train()\n",
        "\n",
        "        # we zero the gradients as they are not removed automatically\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Also, we need to clear out the hidden state of the LSTM,\n",
        "        # detaching it from its history on the last instance.\n",
        "        model_BiLSTMConv.hidden = model_BiLSTMConv.init_hidden()\n",
        "\n",
        "        # Send input data to GPU\n",
        "        embedding = embedding.to(device)\n",
        "        \n",
        "        if target.shape[0] != batch_size:\n",
        "            pad = torch.zeros(batch_size - target.shape[0])\n",
        "            target = torch.cat((target, pad), dim=0)\n",
        "            \n",
        "        # squeeze is needed as the predictions are initially size (batch size, 1) and we need to remove the dimension of size 1\n",
        "        # Have to transpose batch and sequence dimensions for nn.LSTM\n",
        "        predictions = model_BiLSTMConv(embedding).squeeze(1)\n",
        "        loss = nn.CrossEntropyLoss()(predictions, target.long().to(device))\n",
        "        loss_history.append(float(loss))\n",
        "        \n",
        "        predictions = torch.softmax(predictions.detach(), dim=1)\n",
        "        predictions = np.argmax(predictions.cpu().numpy(), axis=1)\n",
        "        target = target.detach()\n",
        "        acc_history.append(accuracy(predictions.astype(int), \n",
        "                                    target.cpu().numpy().astype(int)))     \n",
        "\n",
        "        # calculate the gradient of each parameter\n",
        "        loss.backward()\n",
        "\n",
        "        # update the parameters using the gradients and optimizer algorithm\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = np.array(loss_history).mean()\n",
        "    epoch_acc = np.array(acc_history).mean()\n",
        "    \n",
        "    \n",
        "    val_acc, cm, recall, precision, f1 = eval_lstm2(model_BiLSTMConv, valid_loader, batch_size)\n",
        "    print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.3f} | Train Acc: {epoch_acc:.2f}%' )\n",
        "    print(f'---> Valid accuracy : {val_acc:.2f}%')"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/utils/data/sampler.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.weights = torch.tensor(weights, dtype=torch.double)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| Epoch: 01 | Train Loss: 1.095 | Train Acc: 38.79%\n",
            "---> Valid accuracy : 57.14%\n",
            "| Epoch: 02 | Train Loss: 1.047 | Train Acc: 48.94%\n",
            "---> Valid accuracy : 63.64%\n",
            "| Epoch: 03 | Train Loss: 0.986 | Train Acc: 54.67%\n",
            "---> Valid accuracy : 68.31%\n",
            "| Epoch: 04 | Train Loss: 0.968 | Train Acc: 56.19%\n",
            "---> Valid accuracy : 69.87%\n",
            "| Epoch: 05 | Train Loss: 0.962 | Train Acc: 56.38%\n",
            "---> Valid accuracy : 61.56%\n",
            "| Epoch: 06 | Train Loss: 0.964 | Train Acc: 56.06%\n",
            "---> Valid accuracy : 64.68%\n",
            "| Epoch: 07 | Train Loss: 0.936 | Train Acc: 61.15%\n",
            "---> Valid accuracy : 66.23%\n",
            "| Epoch: 08 | Train Loss: 0.934 | Train Acc: 60.08%\n",
            "---> Valid accuracy : 67.27%\n",
            "| Epoch: 09 | Train Loss: 0.907 | Train Acc: 63.56%\n",
            "---> Valid accuracy : 71.17%\n",
            "| Epoch: 10 | Train Loss: 0.899 | Train Acc: 64.59%\n",
            "---> Valid accuracy : 70.65%\n",
            "| Epoch: 11 | Train Loss: 0.902 | Train Acc: 65.30%\n",
            "---> Valid accuracy : 67.01%\n",
            "| Epoch: 12 | Train Loss: 0.881 | Train Acc: 67.53%\n",
            "---> Valid accuracy : 67.79%\n",
            "| Epoch: 13 | Train Loss: 0.868 | Train Acc: 68.49%\n",
            "---> Valid accuracy : 67.01%\n",
            "| Epoch: 14 | Train Loss: 0.857 | Train Acc: 70.78%\n",
            "---> Valid accuracy : 67.79%\n",
            "| Epoch: 15 | Train Loss: 0.866 | Train Acc: 69.33%\n",
            "---> Valid accuracy : 67.53%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tVpz5o8pLkcJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ee230331-31e6-43ff-c81f-8ff4b5e83bab"
      },
      "cell_type": "code",
      "source": [
        "acc_test, cm_test, recall_test, precision_test, f1_test = eval_lstm2(model_BiLSTMConv, test_loader, batch_size)\n",
        "print(\"Accuracy on test dataset : %.2f\" % acc_test, \"%\")\n",
        "print(\"F1-measure on test dataset : \", f1_test)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test dataset : 70.39 %\n",
            "F1-measure on test dataset :  [0.22222222 0.82452431 0.62037037]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9LbDmN0bMKVS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Test task C"
      ]
    },
    {
      "metadata": {
        "id": "5A-40g1YMMDP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#  <-------------- Dataset management -------------> #\n",
        "# Get the vocabulary back\n",
        "with open(\"test_vocabulary_c.txt\", \"r\") as file:\n",
        "    vocabulary_c = file.read().splitlines()\n",
        "\n",
        "# Get the cleaned tokenized corpus back\n",
        "with open(\"test_corpus_tweets_c.txt\", \"r\") as file:\n",
        "    tmp_c = file.read().splitlines()\n",
        "\n",
        "# In each sentence, we get rid of the last token, which is '\\n'\n",
        "clean_corpus_c = [sentence[:-1] for sentence in tmp_c]\n",
        "\n",
        "tokenized_corpus_c = [[token for token in sentence.split(' ')][:-1] for sentence in tmp_c]\n",
        "\n",
        "#  <-------------- END Dataset management -------------> #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v6Ea-6V0ND3-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_path = 'emb_dic_c.txt'\n",
        "emb_dict_c = {}\n",
        "glove = open(embedding_path)\n",
        "for line in glove:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    try:\n",
        "        vector = np.asarray(values[1:], dtype='float32')\n",
        "        if len(vector) != 100:\n",
        "          print(word, len(vector))\n",
        "        emb_dict_c[word] = vector\n",
        "    except:\n",
        "        print(\"Parsing problem on word \", word, \" discarding it\")\n",
        "glove.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hw24j5w1NKoZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4b4d6cda-6aa2-4ef1-fc9c-60c8acdf0758"
      },
      "cell_type": "code",
      "source": [
        "emb_corpus_c = embed_corpus_2(emb_dict_c, tokenized_corpus_c)"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of not recognised words (those we do not have an embedding for) : 6.04 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UT3xFqQBNUB1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3ab262dd-fa32-4674-d9c5-727869923aed"
      },
      "cell_type": "code",
      "source": [
        "print(emb_corpus_c.shape)\n",
        "\n",
        "# We need to make it a multiple of the batchsize:\n",
        "\n",
        "pad = torch.zeros((11, 58, 100))\n",
        "emb_corpus_c = torch.cat((emb_corpus_c, pad), dim=0)\n",
        "print(emb_corpus_c.shape)\n",
        "# It is already a mutliple of batchsize 16"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([213, 58, 100])\n",
            "torch.Size([224, 58, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2Oo1wCO1OFM9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Make batches of 16 : \n",
        "\n",
        "input_c = emb_corpus_c.view(-1, 16, 58, 100)\n",
        "\n",
        "# Preparing a container for the results\n",
        "prediction_c = np.zeros((int(224/16),16))\n",
        "\n",
        "for batch in range(14):\n",
        "    output = model_BiLSTMConv(torch.Tensor(input_c[batch]).to(device))\n",
        "    output = torch.softmax(output, dim=1)\n",
        "    output = np.argmax(output.cpu().detach().numpy(), axis =1)\n",
        "    prediction_c[batch] = output\n",
        "\n",
        "prediction_c = torch.Tensor(prediction_c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8-Aj-INiPjfJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "ea6f4245-e3ff-4517-fcde-9ddf260d603f"
      },
      "cell_type": "code",
      "source": [
        "# reshape prediction_a and dicard the last elements corresponding to the padding\n",
        "prediction_c = prediction_c.view(-1)[:-11]\n",
        "print(prediction_c.shape)\n",
        "\n",
        "# Getting back the np.array, and round the result to have 0 or 1\n",
        "prediction_c = prediction_c.numpy()\n",
        "prediction_c = prediction_c.astype(int)\n",
        "print(prediction_c)"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([213])\n",
            "[2 2 1 1 1 2 1 1 1 1 2 2 2 2 0 1 2 0 2 1 0 2 1 1 1 1 2 0 2 1 2 1 1 1 2 2 1\n",
            " 1 1 2 1 1 2 2 2 1 2 0 2 0 1 2 1 2 2 1 2 1 1 2 2 1 2 1 1 2 1 1 2 1 1 2 2 2\n",
            " 1 1 1 1 2 2 0 0 1 1 2 1 2 1 1 1 1 1 1 2 2 2 2 1 2 2 1 1 2 1 1 1 2 1 0 2 1\n",
            " 0 1 2 1 1 1 1 1 2 1 0 2 2 1 1 2 1 2 2 1 1 2 2 2 2 1 1 2 0 2 1 1 2 2 1 1 2\n",
            " 0 1 0 1 1 1 0 1 2 2 1 2 2 1 2 1 1 2 1 2 2 1 1 1 1 1 1 1 0 1 1 2 1 1 1 2 1\n",
            " 1 0 2 2 2 1 2 2 1 2 0 1 2 1 2 1 1 2 1 1 1 2 1 1 2 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iUeVkbZxP1WF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "prediction_c_letters = [\"IND\" if element == 1 else \"GRP\" if element == 2 \n",
        "                        else \"OTH\" for element in prediction_c]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aCAvrvbSQnL8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataframe_c = pd.read_csv('test_set_taskc.tsv', sep=\"\\t\", header=0)\n",
        "id_c = dataframe_c[\"id\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aZ-kq0C5Q-0i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pred_dataframe = pd.DataFrame(prediction_c_letters, index=id_c)\n",
        "pred_dataframe.to_csv(path_or_buf =\"pred_c.csv\", header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}